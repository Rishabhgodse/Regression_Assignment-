{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n81ywSpkWKht"
      },
      "outputs": [],
      "source": [
        "# What is Simple Linear Regression?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regression is a model that describes the relationship between two variables using a straight line. The equation for this line is:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "𝑋\n",
        "+\n",
        "𝑐\n",
        "Y=mX+c\n",
        "Where:\n",
        "\n",
        "𝑌\n",
        "Y is the dependent variable.\n",
        "𝑋\n",
        "X is the independent variable.\n",
        "𝑚\n",
        "m is the slope (rate of change).\n",
        "𝑐\n",
        "c is the intercept (the value of\n",
        "𝑌\n",
        "Y when\n",
        "𝑋\n",
        "=\n",
        "0\n",
        "X=0)"
      ],
      "metadata": {
        "id": "NhTNR20dWR9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the key assumptions of Simple Linear Regression?"
      ],
      "metadata": {
        "id": "eYCwEAtAWboK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linearity: The relationship between X and Y should be linear.\n",
        "\n",
        "Independence: The observations are independent of each other.\n",
        "\n",
        "Homoscedasticity: The variance of residuals should remain constant across values of X.\n",
        "\n",
        "Normality: The residuals should follow a normal distribution.\n",
        "\n",
        "No Multicollinearity: Not applicable for Simple Linear Regression since only one independent variable is used."
      ],
      "metadata": {
        "id": "NRnN8og8W5OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What does the coefficient m represent in the equation Y=mX+c?"
      ],
      "metadata": {
        "id": "2psGpUxpXCyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coefficient\n",
        "𝑚\n",
        "m represents the slope. It indicates how much\n",
        "𝑌\n",
        "Y will change for a one-unit increase in\n",
        "𝑋\n",
        "X."
      ],
      "metadata": {
        "id": "ULyjgD5oXH4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What does the intercept c represent in the equation Y=mX+c?"
      ],
      "metadata": {
        "id": "4xtVAwobXMUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intercept\n",
        "𝑐\n",
        "c is the value of\n",
        "𝑌\n",
        "Y when\n",
        "𝑋\n",
        "=\n",
        "0\n",
        "X=0, representing the starting point of the regression line."
      ],
      "metadata": {
        "id": "omzgcYrhXRr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How do we calculate the slope m in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "DfExYyCIXWEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The slope\n",
        "𝑚\n",
        "m can be calculated using the following formula:\n",
        "\n",
        "𝑚\n",
        "=\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "‾\n",
        ")\n",
        "(\n",
        "𝑌\n",
        "𝑖\n",
        "−\n",
        "𝑌\n",
        "‾\n",
        ")\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "‾\n",
        ")\n",
        "2\n",
        "m=\n",
        "∑(X\n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        " )\n",
        "2\n",
        "\n",
        "∑(X\n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        " )(Y\n",
        "i\n",
        "​\n",
        " −\n",
        "Y\n",
        " )\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑋\n",
        "𝑖\n",
        "X\n",
        "i\n",
        "​\n",
        "  and\n",
        "𝑌\n",
        "𝑖\n",
        "Y\n",
        "i\n",
        "​\n",
        "  are the individual data points,\n",
        "𝑋\n",
        "‾\n",
        "X\n",
        "  is the mean of X,\n",
        "𝑌\n",
        "‾\n",
        "Y\n",
        "  is the mean of Y.\n",
        "\n",
        "  import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample Data: X (independent variable), Y (dependent variable)\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Reshaping for sklearn\n",
        "Y = np.array([1, 2, 3, 4, 5])\n",
        "\n",
        "# 1. Fit Simple Linear Regression using Scikit-learn\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# 2. Coefficients\n",
        "m = model.coef_[0]  # Slope (m)\n",
        "c = model.intercept_  # Intercept (c)\n",
        "\n",
        "print(f\"Slope (m): {m}\")\n",
        "print(f\"Intercept (c): {c}\")\n",
        "\n",
        "# 3. Predict Y values\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "# 4. Plot the regression line\n",
        "plt.scatter(X, Y, color='blue')  # Original data points\n",
        "plt.plot(X, Y_pred, color='red')  # Regression line\n",
        "plt.title('Simple Linear Regression')\n",
        "plt.xlabel('X (Independent Variable)')\n",
        "plt.ylabel('Y (Dependent Variable)')\n",
        "plt.show()\n",
        "\n",
        "The slope\n",
        "𝑚\n",
        "m, which represents how much\n",
        "𝑌\n",
        "Y changes for every one-unit change in\n",
        "𝑋\n",
        "X.\n",
        "The intercept\n",
        "𝑐\n",
        "c, which is the value of\n",
        "𝑌\n",
        "Y when\n",
        "𝑋\n",
        "=\n",
        "0\n",
        "X=0."
      ],
      "metadata": {
        "id": "DNzGMBm9XbuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the purpose of the least squares method in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "EcNAoIGvXwDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Least Squares Method minimizes the sum of the squared differences between the actual and predicted values of\n",
        "𝑌\n",
        "Y. This method finds the best-fitting line by reducing the overall error, i.e., the squared difference between the observed and predicted values of\n",
        "𝑌\n",
        "Y.\n",
        "\n",
        "In mathematical terms, we minimize:\n",
        "\n",
        "SSE\n",
        "=\n",
        "∑\n",
        "(\n",
        "𝑌\n",
        "𝑖\n",
        "−\n",
        "𝑌\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "2\n",
        "SSE=∑(Y\n",
        "i\n",
        "​\n",
        " −\n",
        "Y\n",
        "^\n",
        "  \n",
        "i\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "Where\n",
        "𝑌\n",
        "𝑖\n",
        "Y\n",
        "i\n",
        "​\n",
        "  is the actual value, and\n",
        "𝑌\n",
        "^\n",
        "𝑖\n",
        "Y\n",
        "^\n",
        "  \n",
        "i\n",
        "​\n",
        "  is the predicted value based on the regression line."
      ],
      "metadata": {
        "id": "zKTX5Pe9X2aO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How is the coefficient of determination (R²) interpreted in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "CYoKeT-qX9bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The R² (R-squared) value tells us how well the independent variable\n",
        "𝑋\n",
        "X explains the variance in the dependent variable\n",
        "𝑌\n",
        "Y. It is calculated as:\n",
        "\n",
        "𝑅\n",
        "2\n",
        "=\n",
        "1\n",
        "−\n",
        "𝑆\n",
        "𝑆\n",
        "residual\n",
        "𝑆\n",
        "𝑆\n",
        "total\n",
        "R\n",
        "2\n",
        " =1−\n",
        "SS\n",
        "total\n",
        "​\n",
        "\n",
        "SS\n",
        "residual\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑆\n",
        "𝑆\n",
        "residual\n",
        "SS\n",
        "residual\n",
        "​\n",
        "  is the sum of squared residuals (errors),\n",
        "𝑆\n",
        "𝑆\n",
        "total\n",
        "SS\n",
        "total\n",
        "​\n",
        "  is the total sum of squares (the total variance in the dependent variable).\n",
        "An R² value of 1 means the model perfectly fits the data, while an R² value of 0 means the model does not explain any variance.\n",
        "\n",
        "You can compute the R² score using scikit-learn:\n",
        "\n",
        "r_squared = model.score(X, Y)\n",
        "print(f\"R-squared: {r_squared}\")\n"
      ],
      "metadata": {
        "id": "IBfpLbY3YDvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "P9PY7ubsYNQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Linear Regression is an extension of Simple Linear Regression that models the relationship between one dependent variable and multiple independent variables. The equation for Multiple Linear Regression is:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "Where\n",
        "𝑏\n",
        "0\n",
        "b\n",
        "0\n",
        "​\n",
        "  is the intercept, and\n",
        "𝑏\n",
        "1\n",
        ",\n",
        "𝑏\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑏\n",
        "𝑛\n",
        "b\n",
        "1\n",
        "​\n",
        " ,b\n",
        "2\n",
        "​\n",
        " ,…,b\n",
        "n\n",
        "​\n",
        "  are the coefficients for the independent variables\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,…,X\n",
        "n\n",
        "​\n",
        " ."
      ],
      "metadata": {
        "id": "huPFCKNNYVZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the main difference between Simple and Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "SG5-FES5YZfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key difference is that Simple Linear Regression uses only one independent variable, while Multiple Linear Regression involves two or more independent variables."
      ],
      "metadata": {
        "id": "HnYbVyUDYjtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the key assumptions of Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "4aEP3Q69YnOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linearity: The relationship between the dependent variable and each independent variable is linear.\n",
        "Independence of errors: The residuals should be independent.\n",
        "Homoscedasticity: The variance of residuals should be constant.\n",
        "Normality of residuals: The residuals should follow a normal distribution.\n",
        "No multicollinearity: The independent variables should not be highly correlated with each other."
      ],
      "metadata": {
        "id": "rR5GkX7UYtYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?"
      ],
      "metadata": {
        "id": "S5GsS2EcYyIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heteroscedasticity refers to the situation where the variance of the residuals is not constant across all levels of the independent variables. This violates the assumption of homoscedasticity and can affect the results of a regression model by making the estimates of the coefficients less reliable."
      ],
      "metadata": {
        "id": "nukO0c0ZY7op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How can you improve a Multiple Linear Regression model with high multicollinearity?"
      ],
      "metadata": {
        "id": "_gISSZlfaRrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To deal with multicollinearity, you can:\n",
        "\n",
        "Remove highly correlated independent variables.\n",
        "Use dimensionality reduction techniques like Principal Component Analysis (PCA).\n",
        "Regularize the model using Ridge Regression or Lasso Regression."
      ],
      "metadata": {
        "id": "JGor_QmLaXcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What are some common techniques for transforming categorical variables for use in regression models?"
      ],
      "metadata": {
        "id": "yLABhGdgac0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common techniques include:\n",
        "\n",
        "One-hot encoding: Creating binary (0 or 1) columns for each category.\n",
        "Label encoding: Assigning each category a unique integer."
      ],
      "metadata": {
        "id": "332iJY5uaiQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the role of interaction terms in Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "qQczqNvBamnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Multiple Linear Regression, interaction terms allow you to model the effect of one independent variable on the dependent variable that depends on the value of another independent variable."
      ],
      "metadata": {
        "id": "UjvlwM2Vasz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How can the interpretation of intercept differ between Simple and Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "IyQIW8VBax8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Simple Linear Regression, the intercept represents the expected value of\n",
        "𝑌\n",
        "Y when\n",
        "𝑋\n",
        "=\n",
        "0\n",
        "X=0. In Multiple Linear Regression, the intercept is the expected value of\n",
        "𝑌\n",
        "Y when all the independent variables are set to 0."
      ],
      "metadata": {
        "id": "OruukzGwa4aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the significance of the slope in regression analysis, and how does it affect predictions?"
      ],
      "metadata": {
        "id": "y7aA88F2a8uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In regression analysis, the slope (denoted as\n",
        "𝑚\n",
        "m) measures the rate of change of the dependent variable\n",
        "𝑌\n",
        "Y in response to a one-unit change in the independent variable\n",
        "𝑋\n",
        "X. It signifies how much\n",
        "𝑌\n",
        "Y will increase (or decrease) when\n",
        "𝑋\n",
        "X increases by one unit. The slope determines the direction and strength of the relationship between the variables:\n",
        "\n",
        "If\n",
        "𝑚\n",
        ">\n",
        "0\n",
        "m>0, the relationship is positive, meaning\n",
        "𝑌\n",
        "Y increases as\n",
        "𝑋\n",
        "X increases.\n",
        "If\n",
        "𝑚\n",
        "<\n",
        "0\n",
        "m<0, the relationship is negative, meaning\n",
        "𝑌\n",
        "Y decreases as\n",
        "𝑋\n",
        "X increases.\n",
        "A larger magnitude of the slope means a steeper line and stronger change in\n",
        "𝑌\n",
        "Y for a unit change in\n",
        "𝑋\n",
        "X."
      ],
      "metadata": {
        "id": "0AWj7Iwwbllw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How does the intercept in a regression model provide context for the relationship between variables?"
      ],
      "metadata": {
        "id": "_WeECO9Ibo35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In regression, the intercept represents the value of the dependent variable\n",
        "𝑌\n",
        "Y when the independent variable\n",
        "𝑋\n",
        "X is zero. It provides a baseline from which the effect of\n",
        "𝑋\n",
        "X starts. Here's a simple linear regression example:\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data (Years of Experience vs Salary)\n",
        "X = np.array([0, 1, 2, 3, 4, 5]).reshape(-1, 1)  # Independent variable (Experience in years)\n",
        "Y = np.array([25000, 28000, 31000, 34000, 37000, 40000])  # Dependent variable (Salary)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Get intercept and slope\n",
        "intercept = model.intercept_\n",
        "slope = model.coef_\n",
        "\n",
        "print(f\"Intercept: {intercept}, Slope: {slope}\")\n",
        "\n",
        "# Prediction and plot\n",
        "Y_pred = model.predict(X)\n",
        "plt.scatter(X, Y, color='blue')\n",
        "plt.plot(X, Y_pred, color='red')\n",
        "plt.title('Simple Linear Regression')\n",
        "plt.xlabel('Years of Experience')\n",
        "plt.ylabel('Salary')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kQp_pIKXbvLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the limitations of using R² as a sole measure of model performance?"
      ],
      "metadata": {
        "id": "DxkgONYYeJw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "R² (the coefficient of determination) indicates how well the independent variables explain the variance in the dependent variable. However, relying solely on R² has some limitations:\n",
        "\n",
        "Overfitting: A higher R² does not always imply a better model. Adding more variables can artificially inflate R², even if they are irrelevant.\n",
        "Ignores Bias-Variance Tradeoff: R² only focuses on the goodness-of-fit and doesn’t account for the tradeoff between model complexity and overfitting.\n",
        "No Penalty for Complexity: Unlike Adjusted R², R² doesn't penalize models for adding more predictors, even if they don't improve the model substantially.\n",
        "Cannot Compare Non-nested Models: R² only works well when comparing models that are nested (one model is a special case of the other) but fails for different model types (like linear vs. non-linear)."
      ],
      "metadata": {
        "id": "6MnpsrtleQqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How would you interpret a large standard error for a regression coefficient?"
      ],
      "metadata": {
        "id": "mkhYt5ahebre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A large standard error for a regression coefficient indicates that the estimate of that coefficient is highly uncertain. It suggests that the coefficient could vary widely across different samples, reducing confidence in its reliability. This could be due to multicollinearity (high correlation between independent variables), a small sample size, or noisy data. A large standard error leads to wider confidence intervals for the coefficient, indicating that the model’s predictions may not be very precise."
      ],
      "metadata": {
        "id": "3J6oZ_JUehYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How can heteroscedasticity be identified in residual plots, and why is it important to address it?"
      ],
      "metadata": {
        "id": "eWCzolM7elom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is an extension of linear regression that models the relationship between the independent variable\n",
        "𝑋\n",
        "X and the dependent variable\n",
        "𝑌\n",
        "Y as an\n",
        "𝑛\n",
        "n-degree polynomial. The model takes the form:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X+b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "\n",
        "It introduces non-linearity into the model by adding higher-order terms (like\n",
        "𝑋\n",
        "2\n",
        "X\n",
        "2\n",
        " ,\n",
        "𝑋\n",
        "3\n",
        "X\n",
        "3\n",
        " , etc.) to better fit data that has a curvilinear or more complex relationship."
      ],
      "metadata": {
        "id": "4vyfKKEcerDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?"
      ],
      "metadata": {
        "id": "C939VIHxeu8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A high R² but a low Adjusted R² indicates that while the model explains a significant amount of variance in the dependent variable, the inclusion of additional predictors might not improve the model substantially. Adjusted R² accounts for the number of predictors in the model and penalizes for adding irrelevant or insignificant predictors. If Adjusted R² is significantly lower than R², it suggests that some of the variables do not contribute meaningfully to the model and may even introduce noise."
      ],
      "metadata": {
        "id": "O-06N8YHe0o7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Why is it important to scale variables in Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "eKndC4nnfPFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling is crucial in Multiple Linear Regression for several reasons:\n",
        "\n",
        "Ensures Comparability: When predictors are on different scales (e.g., age in years vs. income in dollars), it can make the coefficients hard to interpret and can skew the results.\n",
        "Improves Algorithm Performance: Many algorithms, like gradient-based optimization used in regression, converge faster when the input data is scaled. Variables with large magnitudes can dominate the model.\n",
        "Reduces Multicollinearity: Scaling helps reduce issues of multicollinearity by normalizing the magnitude of each feature.\n",
        "Regularization: In regularized regression methods like Ridge or Lasso, scaling is critical because penalties applied to coefficients depend on their magnitude."
      ],
      "metadata": {
        "id": "0aT118hGfWD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is polynomial regression?"
      ],
      "metadata": {
        "id": "RSVo40Q2fbOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is a type of regression analysis where the relationship between the independent variable\n",
        "𝑋\n",
        "X and the dependent variable\n",
        "𝑌\n",
        "Y is modeled as an\n",
        "𝑛\n",
        "n-degree polynomial. It is an extension of linear regression, which fits a linear relationship, to capture non-linear patterns by using polynomial terms (e.g.,\n",
        "𝑋\n",
        "2\n",
        "X\n",
        "2\n",
        " ,\n",
        "𝑋\n",
        "3\n",
        "X\n",
        "3\n",
        " ).\n",
        "\n",
        "The general form of a polynomial regression model is:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X+b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "\n",
        "𝑌\n",
        "Y is the dependent variable.\n",
        "𝑋\n",
        "X is the independent variable.\n",
        "𝑏\n",
        "0\n",
        ",\n",
        "𝑏\n",
        "1\n",
        ",\n",
        "𝑏\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑏\n",
        "𝑛\n",
        "b\n",
        "0\n",
        "​\n",
        " ,b\n",
        "1\n",
        "​\n",
        " ,b\n",
        "2\n",
        "​\n",
        " ,…,b\n",
        "n\n",
        "​\n",
        "  are the coefficients of the polynomial.\n",
        "\n",
        "  When the data exhibits curvature or a non-linear trend that a straight line cannot adequately capture.\n",
        "For relationships that may have a U-shape or other complex, non-linear patterns.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data (X: years, Y: performance score)\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)\n",
        "Y = np.array([3, 6, 10, 15, 21, 30, 42, 56, 72])\n",
        "\n",
        "# Transform features into polynomial features (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit the Polynomial Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "\n",
        "# Prediction and plot\n",
        "Y_pred = model.predict(X_poly)\n",
        "plt.scatter(X, Y, color='blue')\n",
        "plt.plot(X, Y_pred, color='red')\n",
        "plt.title('Polynomial Regression (Degree 2)')\n",
        "plt.xlabel('X (Independent Variable)')\n",
        "plt.ylabel('Y (Dependent Variable)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Dw0GUpTufhcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How does polynomial regression differ from linear regression?"
      ],
      "metadata": {
        "id": "1AISdSINf6jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression assumes a linear relationship between the independent and dependent variables, with the equation:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X\n",
        "Polynomial Regression, on the other hand, models non-linear relationships by introducing higher-degree terms of the independent variable:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X+b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "\n",
        "Key differences:\n",
        "\n",
        "Linear Regression fits a straight line to the data, while Polynomial Regression fits a curve.\n",
        "Polynomial regression can model more complex, non-linear relationships between the variables, allowing for better fitting when the relationship is not a straight line.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([1, 4, 9, 16, 25])\n",
        "\n",
        "# Transform features into polynomial features\n",
        "poly = PolynomialFeatures(degree=2)  # Degree 2 polynomial\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit Polynomial Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "\n",
        "# Predictions\n",
        "Y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the original data and polynomial fit\n",
        "plt.scatter(X, Y, color='blue')\n",
        "plt.plot(X, Y_pred, color='red')\n",
        "plt.title('Polynomial Regression')\n",
        "plt.xlabel('X (Independent Variable)')\n",
        "plt.ylabel('Y (Dependent Variable)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WXcpEF36gCCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# When is polynomial regression used?"
      ],
      "metadata": {
        "id": "vEa-OqT4gOdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is used when the relationship between the independent variable(s) and the dependent variable is non-linear, but can be approximated by a polynomial function. It is applied when data shows curvature or follows a more complex pattern that cannot be captured by a simple straight line, like U-shaped or exponential trends."
      ],
      "metadata": {
        "id": "ylHqhICngzp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the general equation for polynomial regression?"
      ],
      "metadata": {
        "id": "jEmnZg5Xg2sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The general form of a polynomial regression equation is:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝑏\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X+b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +b\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑌\n",
        "Y is the dependent variable.\n",
        "𝑋\n",
        "X is the independent variable.\n",
        "𝑏\n",
        "0\n",
        ",\n",
        "𝑏\n",
        "1\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑏\n",
        "𝑛\n",
        "b\n",
        "0\n",
        "​\n",
        " ,b\n",
        "1\n",
        "​\n",
        " ,…,b\n",
        "n\n",
        "​\n",
        "  are the coefficients of the polynomial.\n",
        "𝑛\n",
        "n is the degree of the polynomial."
      ],
      "metadata": {
        "id": "njamK_oPg68A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Can polynomial regression be applied to multiple variables?"
      ],
      "metadata": {
        "id": "7zjqun-lhAaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, polynomial regression can be applied to multiple variables (features). When extending to multiple variables, each predictor variable can have polynomial terms, creating an interaction between different degrees of the variables. For example, with two independent variables\n",
        "𝑋\n",
        "1\n",
        "X\n",
        "1\n",
        "​\n",
        "  and\n",
        "𝑋\n",
        "2\n",
        "X\n",
        "2\n",
        "​\n",
        " , a second-degree polynomial regression might look like:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝑏\n",
        "3\n",
        "𝑋\n",
        "1\n",
        "2\n",
        "+\n",
        "𝑏\n",
        "4\n",
        "𝑋\n",
        "2\n",
        "2\n",
        "+\n",
        "𝑏\n",
        "5\n",
        "𝑋\n",
        "1\n",
        "𝑋\n",
        "2\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +b\n",
        "3\n",
        "​\n",
        " X\n",
        "1\n",
        "2\n",
        "​\n",
        " +b\n",
        "4\n",
        "​\n",
        " X\n",
        "2\n",
        "2\n",
        "​\n",
        " +b\n",
        "5\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        "\n",
        "known as multivariate polynomial regression"
      ],
      "metadata": {
        "id": "2yjQvMClhGJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the limitations of polynomial regression?"
      ],
      "metadata": {
        "id": "fpmzC38BhJ0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting: Higher-degree polynomials can fit the training data too well, capturing noise and leading to poor generalization on new data.\n",
        "Complexity: As the degree increases, the model becomes more complex and harder to interpret.\n",
        "Extrapolation Issues: Polynomial regression is sensitive to extrapolation (predicting values outside the observed data range), often leading to unrealistic predictions.\n",
        "Multicollinearity: Higher-degree terms of the same variable (e.g.,\n",
        "𝑋\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        "X,X\n",
        "2\n",
        " ,X\n",
        "3\n",
        " ) can be highly correlated, leading to multicollinearity issues, which make coefficient estimates unstable."
      ],
      "metadata": {
        "id": "nfLzD8ZthT3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What methods can be used to evaluate model fit when selecting the degree of a polynomial?"
      ],
      "metadata": {
        "id": "Kqm7AWA-hYT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Validation: Splitting the dataset into training and validation sets to check how well the model generalizes to unseen data.\n",
        "Adjusted\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        " : Adjusted\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  takes into account the number of predictors and penalizes adding unnecessary terms. It helps balance model complexity and fit.\n",
        "AIC/BIC (Akaike/Bayesian Information Criterion): These methods provide a tradeoff between goodness-of-fit and model complexity. Lower values indicate better models.\n",
        "Mean Squared Error (MSE): Evaluating the average squared difference between observed and predicted values."
      ],
      "metadata": {
        "id": "pqF74njhhe2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Why is visualization important in polynomial regression?"
      ],
      "metadata": {
        "id": "9gSQgQl9hizg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization is important because:\n",
        "\n",
        "It helps assess the fit of the polynomial model by comparing the actual data points to the predicted curve.\n",
        "It aids in detecting overfitting or underfitting: An overly complex model might follow the data too closely, while an overly simple model may fail to capture trends.\n",
        "Visualization makes it easier to choose the correct degree of the polynomial by showing how the curve fits the data."
      ],
      "metadata": {
        "id": "fXhZUx2bhm9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How is polynomial regression implemented in Python?"
      ],
      "metadata": {
        "id": "Nep_NMFUhrHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression in Python is typically implemented using PolynomialFeatures from sklearn.preprocessing to create polynomial features and then applying linear regression. Here is an example:\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data (X and Y)\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)\n",
        "Y = np.array([1, 4, 9, 16, 25, 36, 49, 64, 81])\n",
        "\n",
        "# Create polynomial features (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit the polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "\n",
        "# Predictions\n",
        "Y_pred = model.predict(X_poly)\n",
        "\n",
        "# Visualization\n",
        "plt.scatter(X, Y, color='blue')  # Original data points\n",
        "plt.plot(X, Y_pred, color='red')  # Fitted polynomial curve\n",
        "plt.title('Polynomial Regression (Degree 2)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.show()\n",
        "\n",
        "PolynomialFeatures(degree=2): Transforms the input data\n",
        "𝑋\n",
        "X into second-degree polynomial features (e.g.,\n",
        "𝑋\n",
        "X,\n",
        "𝑋\n",
        "2\n",
        "X\n",
        "2\n",
        " ).\n",
        "\n",
        "LinearRegression: Fits the transformed data using linear regression.\n",
        "\n",
        "Visualization: A plot is generated showing the original data points and the polynomial curve."
      ],
      "metadata": {
        "id": "3V0c1hEZhwZZ"
      }
    }
  ]
}